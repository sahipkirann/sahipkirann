name: Deploy MkDocs to GitHub Pages
on:
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      - run: python -m pip install --upgrade pip
      - run: |
          pip install mkdocs "mkdocs-material[imaging]" \
          mkdocs-roamlinks-plugin mkdocs-glightbox mkdocs-minify-plugin \
          mkdocs-rss-plugin mkdocs-redirects mkdocs-awesome-pages-plugin \
          mkdocs-git-revision-date-localized-plugin
      - name: Fix Obsidian image embeds -> Markdown
        run: |
            python - <<'PY'
            import re
            from pathlib import Path

            docs = Path('docs')
            imgs = {}
            for p in docs.rglob('*'):
                if p.is_file() and p.suffix.lower() in ('.png','.jpg','.jpeg','.gif','.webp','.svg'):
                    imgs[p.name.lower()] = p.relative_to(docs).as_posix()

            for md in docs.rglob('*.md'):
                text = md.read_text(encoding='utf-8')
                def repl(m):
                    raw = m.group(1)
                    fname = raw.split('|')[0].strip()
                    path = imgs.get(fname.lower())
                    if path:
                        return f'![](/{path})'
                    return f'![{fname}]({fname})'
                new = re.sub(r'!\[\[([^\]]+?\.(?:png|jpe?g|gif|webp|svg)(?:\|[^\]]*)?)\]\]', repl, text, flags=re.I)
                if new != text:
                    md.write_text(new, encoding='utf-8')
            PY
      - name: SEO + titles normalize (front-matter + H1)
        run: |
          python - <<'PY'
          import re
          from pathlib import Path

          def parse_fm(lines):
              i=0; s=e=-1
              while i<len(lines) and not lines[i].strip(): i+=1
              if i<len(lines) and lines[i].strip()=='---':
                  s=i; i+=1
                  while i<len(lines) and lines[i].strip()!='---': i+=1
                  if i<len(lines) and lines[i].strip()=='---':
                      e=i; i+=1
              return s,e,i

          def has_key(lines,s,e,key):
              if s==-1: return False
              for k in range(s+1,e):
                  if re.match(rf'^\s*{key}\s*:', lines[k], flags=re.I): return True
              return False

          def upsert(lines,s,e,c,key,val):
              line=f'{key}: {val}'
              if s!=-1:
                  lines.insert(s+1,line); e+=1; c+=1
              else:
                  lines=['---',line,'---','']+lines; s,e,c=0,2,4
              return lines,s,e,c

          def strip_md(t):
              t=re.sub(r'```.*?```','',t,flags=re.S)
              t=re.sub(r'`[^`]*`','',t)
              t=re.sub(r'!\[[^\]]*\]\([^)]+\)','',t)
              t=re.sub(r'\[([^\]]+)\]\([^)]+\)',r'\1',t)
              t=re.sub(r'<[^>]+>.*?</[^>]+>','',t,flags=re.S)  
              t=re.sub(r'<[^>]+>','',t)
              t=re.sub(r'\s+',' ',t).strip()
              return t

          root=Path('docs')
          for p in root.rglob('*.md'):
              sp=str(p).replace('\\','/')
              if '/assets/' in sp: continue
              stem=p.stem.replace('-', ' ').strip()

              lines=p.read_text(encoding='utf-8',errors='ignore').splitlines()
              if lines and lines[0].startswith('\ufeff'): lines[0]=lines[0].lstrip('\ufeff')

              s,e,c=parse_fm(lines)

              if not has_key(lines,s,e,'title'):
                  lines,s,e,c=upsert(lines,s,e,c,'title',f'"{stem}"')

              if not has_key(lines,s,e,'description'):
                  body='\n'.join(lines[c:])
                  # quitar H1 visible del body
                  body=re.sub(r'^\s*# .*$', '', body, flags=re.M)
                  # elegir primer párrafo útil que no sea JSON/HTML
                  paras=[x.strip() for x in re.split(r'\n\s*\n',body) if x.strip()]
                  desc=''
                  for para in paras:
                      if para.lstrip().startswith(('{','<')):  # evita JSON-LD/HTML
                          continue
                      cand=strip_md(para)
                      if len(cand)>=40:
                          desc=cand; break
                  if not desc and paras:
                      desc=strip_md(paras[0])
                  if desc:
                      if len(desc)>160:
                          cut=desc[:160]; desc=cut[:cut.rfind(' ')] if ' ' in cut else cut
                      lines,s,e,c=upsert(lines,s,e,c,'description',f'"{desc}"')

              fence=re.compile(r'^\s*(`{3,}|~{3,})'); in_code=False
              has_h1=False
              for j in range(c,len(lines)):
                  if fence.match(lines[j]): in_code=not in_code; continue
                  if not in_code and re.match(r'^\s*#\s+.+$', lines[j]):
                      has_h1=True; break
              if not has_h1:
                  # usar el title del FM si lo hay
                  title_line=None
                  if s!=-1:
                      for k in range(s+1,e):
                          m=re.match(r'^\s*title\s*:\s*(.+)$', lines[k], flags=re.I)
                          if m: 
                              title_line=m.group(1).strip().strip('"')
                              break
                  lines.insert(c, f'# {title_line or stem}')

              p.write_text('\n'.join(lines)+'\n',encoding='utf-8')
          PY
      - run: mkdocs build --clean --site-dir site
      - name: Generate sitemap.xml
        run: |
          python - <<'PY'
          import os, time, xml.etree.ElementTree as ET
          base = "https://lautarovculic.github.io"
          urls=[]
          for root,_,files in os.walk("site"):
              for f in files:
                  if not f.endswith(".html"): continue
                  path=os.path.join(root,f)
                  rel=path[len("site/"):]
                  if rel in ("404.html",): continue
                  if rel.startswith(("assets/","search/")): continue
                  url=rel[:-10] if rel.endswith("index.html") else rel
                  if not url.startswith("/"): url="/"+url
                  if url.endswith(".html"): url=url
                  loc=base+url
                  mtime=time.strftime("%Y-%m-%d", time.gmtime(os.path.getmtime(path)))
                  urls.append((loc,mtime))
          urlset=ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
          for loc,mtime in sorted(urls):
              u=ET.SubElement(urlset,"url")
              ET.SubElement(u,"loc").text=loc
              ET.SubElement(u,"lastmod").text=mtime
          ET.ElementTree(urlset).write("site/sitemap.xml", encoding="utf-8", xml_declaration=True)
          PY
      - uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
